import subprocess
import os
import pandas as pd

# Path to the output CSV file generated by the R script
output_csv = "data/files_info_Batches.csv"

# Step 1: Check if the file already exists
if not os.path.exists(output_csv):
    try:
        # Run the R script if the file does not exist
        print("Sequencing batches information not available. Reading the qzipped files and determining that now.")
        subprocess.run(
            f"conda run -p /.snakemake/conda/ee5771f5a5b4f769c2b98bafaa645bd0_ Rscript workflow/scripts/batch.R",
            shell=True, check=True)
        print("Sequencing batches determined. File saved as 'files_info_Batches.csv'")
    except subprocess.CalledProcessError as e:
        print(f"Error occurred while running the R script: {e}")
        exit(1)
else:
    print(f"Sequencing batch  information already exists, as the {output_csv} already exists,  skipping R script execution.")

# Step 2: Read the CSV file (whether generated or already existing)
samples_table = pd.read_csv(output_csv).set_index(["sample", "region"], drop=False)

print(f"Loaded samples table with {len(samples_table)} entries.")


# Internally adjust the `fastq1` and `fastq2` columns
samples_table["fastq1"] = "data/" + samples_table["fastq1"]
samples_table["fastq2"] = "data/" + samples_table["fastq2"]

# Check for column names with leading or trailing whitespaces
def check_column_names(df):
    columns = df.columns
    stripped_columns = [col.strip() for col in columns]
    whitespace_columns = [col for col, stripped_col in zip(columns, stripped_columns) if col != stripped_col]
    return whitespace_columns

whitespace_columns = check_column_names(samples_table)
if whitespace_columns:
    print(f"Warning: The following columns have leading or trailing whitespaces: {whitespace_columns}")

# Get samples from batch and region
def get_samples_from_batch_region(batch, region):
    return samples_table.loc[(samples_table['Batch_ID'] == batch) & (samples_table['region'] == region), 'sample'].tolist()

# Get expected length from batch and region
def get_expected_length_from_batch_region(batch, region):
    lengths = samples_table.loc[
        (samples_table['Batch_ID'] == batch) & (samples_table['region'] == region), 'expected.length'
    ].unique()
    if len(lengths) == 1:
        return lengths[0]
    else:
        raise ValueError(f"Multiple expected lengths found for batch {batch} and region {region}.")


# Define thread management functions
def determine_threads(wildcards):
    total_cores = workflow.cores
    core_multiplier = workflow.attempt
    return total_cores / 2

def all_threads(wildcards):
    total_cores = workflow.cores
    core_multiplier = workflow.attempt
    return total_cores
    
    
    #    return min(2 * core_multiplier, total_cores)
    # This sets a base of 2 threads per job and adjusts based on the attempt number.
    # Adjust this logic as needed for your specific use-case.
    
# Now the samples_table variable has the updated "SampleNum" column and can be used in the rest of your script.

rule trim_primers_with_cutadapt:
    input:
        fq1 = lambda wildcards: samples_table.loc[(wildcards.sample, wildcards.region), 'fastq1'],
        fq2 = lambda wildcards: samples_table.loc[(wildcards.sample, wildcards.region), 'fastq2']
    output:
        touch("data/favabean/{batch}-{region}/.tmp/.{sample}-cutadapt.done")
    log:
        "data/logs/cutadapt-{batch}-{region}-{sample}.log"
    benchmark:
        "data/benchmarks/trim_primers_with_cutadapt-{batch}-{region}-{sample}.txt"
    params:
        primer_5 =      lambda wildcards: samples_table.loc[(wildcards.sample, wildcards.region), 'primer_5'],
        primer_3 =      lambda wildcards: samples_table.loc[(wildcards.sample, wildcards.region), 'primer_3'],
        sampleNum =     lambda wildcards: samples_table.loc[(wildcards.sample, wildcards.region), 'SampleNum'],
        filt      =     config["initial_filter"]
    message: "trim_primers_with_cutadapt — Removing the adaptors for sample {wildcards.sample} from batch: {wildcards.batch}, region: {wildcards.region}"
    conda:
        "../envs/cutadapt.yaml"
    shell:
        """
        primer5=$(echo '{params.primer_5}' | sed 's/;/ -g /g')
        primer3=$(echo '{params.primer_3}' | sed 's/;/ -G /g')
        mkdir -p data/favabean/{wildcards.batch}-{wildcards.region}/initialFilt_discarded/ &&
        mkdir -p data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/ &&
        cutadapt -j 0 --minimum-length {params.filt} -g $primer5 -G $primer3 \
        --too-short-output        data/favabean/{wildcards.batch}-{wildcards.region}/initialFilt_discarded/{wildcards.sample}_S{params.sampleNum}_L001_R1_001.fastq.gz \
        --too-short-paired-output data/favabean/{wildcards.batch}-{wildcards.region}/initialFilt_discarded/{wildcards.sample}_S{params.sampleNum}_L001_R2_001.fastq.gz \
        -o data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/{wildcards.sample}_S{params.sampleNum}_L001_R1_001.fastq.gz \
        -p data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/{wildcards.sample}_S{params.sampleNum}_L001_R2_001.fastq.gz \
        {input.fq1} {input.fq2} > {log} 2>&1 
        """

rule sequence_length_stats:
    input:
        lambda wildcards: expand("data/favabean/{batch}-{region}/.tmp/.{sample}-cutadapt.done",
                                 batch=wildcards.batch,
                                 region=wildcards.region,
                                 sample=get_samples_from_batch_region(wildcards.batch, wildcards.region))
    output:
        R1="data/favabean/{batch}-{region}/.trimParamR1.txt",
        R2="data/favabean/{batch}-{region}/.trimParamR2.txt"
    benchmark:
        "data/benchmarks/sequence_length_stats-{batch}-{region}.txt"
    params:
        filt=config["initial_filter"],
        trim_param=config["trim_param"]
    message: "sequence_length_stats — Calculate descriptive statistics of the R1 and R2 sequence lengths for the samples: {wildcards.batch}, region: {wildcards.region}"
    conda:
        "../envs/seqkit.yaml"
    shell:
        """
            #R1
            echo {params.trim_param} > data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/.R1{params.trim_param}.txt
            cat data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/*R1_001.fastq.gz | gzip -d | seqkit stats > data/favabean/{wildcards.batch}-{wildcards.region}/stats_R1_lengths.txt -T -a
            
            values=$(tail -n 1 data/favabean/{wildcards.batch}-{wildcards.region}/stats_R1_lengths.txt | cut -f7-11)
            echo $values | cut -d' ' -f2 > data/favabean/{wildcards.batch}-{wildcards.region}/.R1_max_len.txt
            echo $values | cut -d' ' -f3 > data/favabean/{wildcards.batch}-{wildcards.region}/.R1_Q1.txt
            echo $values | cut -d' ' -f4 > data/favabean/{wildcards.batch}-{wildcards.region}/.R1_Q2.txt
            echo $values | cut -d' ' -f5 > data/favabean/{wildcards.batch}-{wildcards.region}/.R1_Q3.txt
            
            # Additional processing to determine default value
            x_int=$(printf "%.0f\n" $(cat data/favabean/{wildcards.batch}-{wildcards.region}/.R1_Q1.txt))
            y_int=$(printf "%.0f\n" $(cat data/favabean/{wildcards.batch}-{wildcards.region}/.R1_Q2.txt))
            ten_percent=$((y_int / 10))
            lower_bound=$((y_int - ten_percent))
            upper_bound=$((y_int + ten_percent))
            if [ "$x_int" -ge "$lower_bound" ] && [ "$x_int" -le "$upper_bound" ]; then
                echo $x_int > data/favabean/{wildcards.batch}-{wildcards.region}/.R1_default.txt
            else
                echo $y_int > data/favabean/{wildcards.batch}-{wildcards.region}/.R1_default.txt
            fi
            cp data/favabean/{wildcards.batch}-{wildcards.region}/.R1_{params.trim_param}.txt data/favabean/{wildcards.batch}-{wildcards.region}/.trimParamR1.txt
            
            #R2
            echo {params.trim_param} > data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/.R2_{params.trim_param}.txt
            cat data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/*R2_001.fastq.gz | gzip -d | seqkit stats > data/favabean/{wildcards.batch}-{wildcards.region}/stats_R2_lengths.txt -T -a
            
            values=$(tail -n 1 data/favabean/{wildcards.batch}-{wildcards.region}/stats_R2_lengths.txt | cut -f7-11)
            echo $values | cut -d' ' -f2 > data/favabean/{wildcards.batch}-{wildcards.region}/.R2_max_len.txt
            echo $values | cut -d' ' -f3 > data/favabean/{wildcards.batch}-{wildcards.region}/.R2_Q1.txt
            echo $values | cut -d' ' -f4 > data/favabean/{wildcards.batch}-{wildcards.region}/.R2_Q2.txt
            echo $values | cut -d' ' -f5 > data/favabean/{wildcards.batch}-{wildcards.region}/.R2_Q3.txt
            
            # Additional processing to determine default value
            x_int=$(printf "%.0f\n" $(cat data/favabean/{wildcards.batch}-{wildcards.region}/.R2_Q1.txt))
            y_int=$(printf "%.0f\n" $(cat data/favabean/{wildcards.batch}-{wildcards.region}/.R2_Q2.txt))
            ten_percent=$((y_int / 10))
            lower_bound=$((y_int - ten_percent))
            upper_bound=$((y_int + ten_percent))
            if [ "$x_int" -ge "$lower_bound" ] && [ "$x_int" -le "$upper_bound" ]; then
                echo $x_int > data/favabean/{wildcards.batch}-{wildcards.region}/.R2_default.txt
            else
                echo $y_int > data/favabean/{wildcards.batch}-{wildcards.region}/.R2_default.txt
            fi
            cp data/favabean/{wildcards.batch}-{wildcards.region}/.R2_{params.trim_param}.txt data/favabean/{wildcards.batch}-{wildcards.region}/.trimParamR2.txt

        """

rule filter_length_for_figaro:
    input:
        sampleCutadapt=lambda wildcards: expand("data/favabean/{batch}-{region}/.tmp/.{sample}-cutadapt.done",
                                 batch=wildcards.batch,
                                 region=wildcards.region,
                                 sample=get_samples_from_batch_region(wildcards.batch, wildcards.region)),
        trimParamR1=lambda wildcards: expand("data/favabean/{batch}-{region}/.trimParamR1.txt",
                                 batch=wildcards.batch,
                                 region=wildcards.region),
        trimParamR2=lambda wildcards: expand("data/favabean/{batch}-{region}/.trimParamR2.txt",
                                 batch=wildcards.batch,
                                 region=wildcards.region)
    output:
        touch("data/favabean/{batch}-{region}/.{sample}_seqkit.done")
    log:
        "data/logs/seqkit-{batch}-{region}-{sample}.log"
    message: "filter_length_for_figaro — Filtering Sample: {wildcards.sample} FASTQ files based on the chosen length for R1 and R2 from batch: {wildcards.batch}, region: {wildcards.region}"
    conda:
        "../envs/seqkit.yaml"
    params:
        sampleNum =     lambda wildcards: samples_table.loc[(wildcards.sample, wildcards.region), 'SampleNum']
    benchmark:
        "data/benchmarks/filter_length_for_figaro-{batch}-{region}-{sample}.txt"
    shell:
        """
        mkdir -p data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/filteredForFigaro/{wildcards.sample}
        
        trimParamR1=$(cat {input.trimParamR1})
        trimParamR2=$(cat {input.trimParamR2})
        
        # Cut sequences longer than the chosen trim parameter length, then remove gaps and any sequences shorter than that, then pair the sequences so that only those that are present in both would be kept
        cat data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/{wildcards.sample}_S{params.sampleNum}_L001_R1_001.fastq.gz | gzip -d | seqkit subseq -r 1:$trimParamR1 | seqkit seq --remove-gaps -m $trimParamR1 -M $trimParamR1 > data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/{wildcards.sample}_S{params.sampleNum}_L001_R1_trimmed.fastq
        cat data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/{wildcards.sample}_S{params.sampleNum}_L001_R2_001.fastq.gz | gzip -d | seqkit subseq -r 1:$trimParamR2 | seqkit seq --remove-gaps -m $trimParamR2 -M $trimParamR2 > data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/{wildcards.sample}_S{params.sampleNum}_L001_R2_trimmed.fastq
        seqkit pair -1 data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/{wildcards.sample}_S{params.sampleNum}_L001_R1_trimmed.fastq -2 data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/{wildcards.sample}_S{params.sampleNum}_L001_R2_trimmed.fastq -O data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/filteredForFigaro/{wildcards.sample} --force > {log} 2>&1
        # I think what seqkit pair kept on rewriting the whole folder at every execution, hence why I have each sample write their result in a separate folder. Now let's move its contents output
        mv data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/filteredForFigaro/{wildcards.sample}/* data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/filteredForFigaro/
        rm -r data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/filteredForFigaro/{wildcards.sample}/
        rm data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/{wildcards.sample}_S{params.sampleNum}_L001_R1_trimmed.fastq data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/{wildcards.sample}_S{params.sampleNum}_L001_R2_trimmed.fastq
        """


rule estimate_trim_params_w_figaro:
    input:
        lambda wildcards: [
            f"data/favabean/{wildcards.batch}-{wildcards.region}/.{sample}_seqkit.done"
            for sample in get_samples_from_batch_region(wildcards.batch, wildcards.region)
        ]
    output:
        "data/favabean/{batch}-{region}/figaro/trimParameters.json"
    log:
        R1="data/logs/figaro-{batch}-{region}.log"
    conda:
        "../envs/figaro.yaml"
    message: "estimate_trim_params_w_figaro — calculating the optimal parameters for DADA2 trimming for batch: {wildcards.batch}, region: {wildcards.region}"
    params:
        expected_length = lambda wildcards: get_expected_length_from_batch_region(wildcards.batch, wildcards.region)
    threads:
        determine_threads
    shell:
        """
         python workflow/envs/figaro/figaro/figaro.py -i data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/filteredForFigaro -o data/favabean/{wildcards.batch}-{wildcards.region}/figaro/ -f 1 -r 1 -a {params.expected_length} -F illumina > {log} 2>&1
         # Do some clean up of files that were solely used for figaro parameters generation
         rm -rf data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/filteredForFigaro
        """


# Note that this uses the cutadapt files, and not the figaro ones! Need to change it so that it can do that for each pair of sequences separately instead of per batch
# Use this as your input: touch("data/favabean/{batch}-{region}/.tmp/.{sample}-cutadapt.done")
rule dada2_1_filter_and_trim:
    input:
        "data/favabean/{batch}-{region}/figaro/trimParameters.json"
    output:
        trimFilter=directory("data/favabean/{batch}-{region}/dada2"),
        donefile  = touch("data/favabean/{batch}-{region}/.tmp/.DADA2_trimFilter.done")
    log:
        R1="data/logs/dada2-1trimFilter-{batch}-{region}.log"
    conda:
        "../envs/dada2.yaml"
    params:
        figaro=config["figaro"]
    benchmark:
        "data/benchmarks/dada2_1_filter_and_trim-{batch}-{region}.txt"
    message: "dada2_1_filter_and_trim — trimming and filtering FASTQ files based on Figaro's {params.figaro} chosen method. batch: {wildcards.batch}, region: {wildcards.region}"
    shell:
        """
         Rscript --vanilla workflow/scripts/dada2_1filterAndTrim.R -i {input} -f data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt -o data/favabean/{wildcards.batch}-{wildcards.region}/dada2 -p {params.figaro} -c {threads} >> {log} 2>&1
        """


rule dada2_2_learn_errors_R1:
    input:
        "data/favabean/{batch}-{region}/dada2"
    output:
        "data/favabean/{batch}-{region}/dada2_DADA2Errors-R1.RData"
    log:
        "data/logs/dada2-2learnErrors-{batch}-{region}-R1.log"
    conda:
        "../envs/dada2.yaml"
    params:
        R="R1"
    threads:
        all_threads
    message: "dada2_2_learn_errors — Learning errors for R1. batch: {wildcards.batch}, region: {wildcards.region}"
    benchmark:
        "data/benchmarks/dada2_2_learn_errors-{batch}-{region}-R1.txt"
    shell:
        """
         Rscript --vanilla workflow/scripts/dada2_2LearnErrors.R -i {input} -r {params.R} -c {threads} >> {log} 2>&1
        """

use rule dada2_2_learn_errors_R1 as dada2_2_learn_errors_R2 with:
    input:
        "data/favabean/{batch}-{region}/dada2"
    output:
        "data/favabean/{batch}-{region}/dada2_DADA2Errors-R2.RData"
    log:
        "data/logs/dada2-2learnErrors-{batch}-{region}-R2.log"
    conda:
        "../envs/dada2.yaml"
    params:
        R="R2"
    benchmark:
        "data/benchmarks/dada2_2_learn_errors-{batch}-{region}-R2.txt"
    message: "dada2_2_learn_errors — Learning errors for R2. batch: {wildcards.batch}, region: {wildcards.region}"



rule dada2_3_denoise_R1:
    input:
        "data/favabean/{batch}-{region}/dada2_DADA2Errors-R1.RData"
    output:
        "data/favabean/{batch}-{region}/dada2_DADA2Denoise-R1.RData"
    log:
        "data/logs/dada2-3denoise-{batch}-{region}-R1.log"
    conda:
        "../envs/dada2.yaml"
    params:
        R="R1"
    message: "dada2_3_denoise — Denoising amplicons based on the learned errors for R1 to create ASVs. batch: {wildcards.batch}, region: {wildcards.region}"
    threads:
        all_threads
    benchmark:
        "data/benchmarks/dada2_3_denoise-{batch}-{region}-R1.txt"
    shell:
        """
         Rscript --vanilla workflow/scripts/dada2_3denoise.R -i {input} -c {threads} -r {params.R} >> {log} 2>&1
        """

use rule dada2_3_denoise_R1 as dada2_3_denoise_R2 with:
    input:
        "data/favabean/{batch}-{region}/dada2_DADA2Errors-R2.RData"
    output:
        "data/favabean/{batch}-{region}/dada2_DADA2Denoise-R2.RData"
    log:
        "data/logs/dada2-3denoise-{batch}-{region}-R2.log"
    conda:
        "../envs/dada2.yaml"
    threads:
        determine_threads
    params:
        R="R2"
    benchmark:
        "data/benchmarks/dada2_3_denoise-{batch}-{region}-R2.txt"
    message: "dada2_3_denoise — Denoising amplicons based on the learned errors for R2 to create ASVs. batch: {wildcards.batch}, region: {wildcards.region}"

rule dada2_4_merge_paired_ends:
    input:
        R1="data/favabean/{batch}-{region}/dada2_DADA2Denoise-R1.RData",
        R2="data/favabean/{batch}-{region}/dada2_DADA2Denoise-R2.RData"
    output:
        "data/favabean/{batch}-{region}/seqtab.tsv"
    log:
        "data/logs/dada2-mergePairedEnds-{batch}-{region}.log"
    conda:
        "../envs/dada2.yaml"
    message: "dada2_4_merge_paired_ends — Merging ASV paired ends (R1, R2). batch: {wildcards.batch}, region: {wildcards.region}"
    benchmark:
        "data/benchmarks/dada2_4_merge_paired_ends-{batch}-{region}.txt"
    shell:
        """
         Rscript --vanilla workflow/scripts/dada2_4mergeR1R2.R -i {input.R1} -s {input.R2} >> {log} 2>&1
         rm -rf data/favabean/{wildcards.batch}-{wildcards.region}/.tmp
        """

combinations = [(row['Batch_ID'], row['region']) for _, row in samples_table.drop_duplicates(['Batch_ID', 'region']).iterrows()]

rule all:
    input:
        expand(
            [
                "data/favabean/{batch}-{region}/figaro/trimParameters.json",
                "data/favabean/{batch}-{region}/dada2",
                "data/favabean/{batch}-{region}/.tmp/.DADA2_trimFilter.done",
                # Include other target outputs as needed
            ],
            zip,
            batch=[x[0] for x in combinations],
            region=[x[1] for x in combinations]
        )


rule dada2_5_remove_chimeras:
    input:
        expand(
            "data/favabean/{batch}-{region}/seqtab.tsv",
            zip,
            batch=[combo[0] for combo in combinations],
            region=[combo[1] for combo in combinations]
        )
    output:
        "data/favabean/{region}_chimeraRemoved.RObjects"
    log:
        "data/logs/dada2-ChimeraASVs-{region}.log"
    conda:
        "../envs/dada2.yaml"
    threads:
        all_threads
    message: "dada2_5_remove_chimeras — Combining the results from the different runs, then removing chimeras through a de novo process. region: {wildcards.region}"
    benchmark:
        "data/benchmarks/dada2_5_remove_chimeras-{region}.txt"
    shell:
        """
        Rscript --vanilla workflow/scripts/dada2_5chimera.R -i data/favabean/ -p {wildcards.region} -c {threads} -o {output} >> {log} 2>&1
        """


rule dada2_6_condense:
    input:
        "data/favabean/{region}_chimeraRemoved.RObjects"
    output:
        "data/favabean/{region}_condense.tsv"
    log:
        "data/logs/dada2-condenseASVs-{region}.log"
    conda:
        "../envs/dada2.yaml"
    threads:
        determine_threads
    message: "dada2_6_condense — Condensing ASVs across batches. region: {wildcards.region}"
    benchmark:
        "data/benchmarks/dada2_6_condense-{region}.txt"
    shell:
        """
         Rscript --vanilla workflow/scripts/dada2_6condense.R -i {input} -o {output} -r {wildcards.region} >> {log} 2>&1
        """

def get_urls(db_name, filetype):
    if config["taxonomy_database"][db_name]["use"]:
        if filetype == "ref":
            return config["taxonomy_database"][db_name]["url"]
        elif filetype == "species":
            return config["taxonomy_database"][db_name]["species"]
    return None

rule download_taxonomy_databases:
    output:
        ref     = "data/resources/{db}_ref.fa.gz",
        species = "data/resources/{db}_species.fa.gz"
    params:
        ref_url = lambda wildcards: get_urls(wildcards.db, "ref"),
        species_url = lambda wildcards: get_urls(wildcards.db, "species")
    run:
        if params.ref_url:
            shell("curl -L {params.ref_url} -o {output.ref}")
        if params.species_url:
            shell("curl -L {params.species_url} -o {output.species}")

rule dada2_7_assign_taxonomy:
    input:
        ASVs="data/favabean/{region}_condense.tsv",
        ref =ancient("data/resources/{db}_ref.fa.gz"),
        spec=ancient("data/resources/{db}_species.fa.gz")
    output:
        ASV_table="data/favabean/{region}_{db}_ASV.tsv",
        taxonomy ="data/favabean/{region}_{db}_taxonomy.tsv"
    log:
        "data/logs/dada2-{region}-{db}_taxonomy.log"
    conda:
        "../envs/dada2.yaml"
    threads:
        determine_threads
    message: "dada2_7_assign_taxonomy — Assigning taxonomy to ASVs. region: {wildcards.region} using {wildcards.db} database."
    benchmark:
        "data/benchmarks/dada2_7_assign_taxonomy-{region}-{db}.txt"
    shell:
        """
        Rscript --vanilla workflow/scripts/dada2_7assignTaxonomy.R \
            -i {input.ASVs} \
            -d {input.ref} \
            -s {input.spec} \
            -c {threads} \
            -o {output.ASV_table} \
            -t {output.taxonomy} > {log} 2>&1
        """

# =============================================================================
# SIDLE — Multi-region 16S reconstruction via SMURF EM algorithm
#
# Replaces primer averaging with the SMURF expectation-maximization algorithm
# (Fuber et al. 2021) as implemented in q2-sidle (Debelius et al.),
# ported to a standalone Python package (sidle_standalone).
#
# Automatically invoked when multiple primer regions are detected.
# Primer sequences and region names are read from files_info_Batches.csv.
# SIDLE-specific parameters are read from favabean.yaml under the "sidle:" key.
# The reference database is taken from taxonomy_database (whichever has use: True).
# =============================================================================

# Unique regions across all batches — used to decide whether SIDLE should run
_unique_regions = list(set(combo[1] for combo in combinations))

# ---------------------------------------------------------------------------
# SIDLE derived values — primers & regions from samples_table
# ---------------------------------------------------------------------------
_region_primers = (
    samples_table[['region', 'primer_5', 'primer_3']]
    .drop_duplicates('region')
    .set_index('region')
)
SIDLE_REGIONS = list(_region_primers.index)

# Illumina adapter sequences that may be prepended to biological primers
_ILLUMINA_ADAPTERS = [
    "ACACTCTTTCCCTACACGACGCTCTTCCGATCT",   # TruSeq Read 1
    "GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT",  # TruSeq Read 2
]

def _strip_adapter(primer_str):
    """Remove Illumina adapter prefix from a primer sequence, if present."""
    s = str(primer_str).split(';')[0].strip().upper()
    for adapter in _ILLUMINA_ADAPTERS:
        if s.startswith(adapter):
            return s[len(adapter):]
    return s

def _get_fwd_primer(region):
    """Get forward biological primer for a region (adapter stripped)."""
    return _strip_adapter(_region_primers.loc[region, 'primer_5'])

def _get_rev_primer(region):
    """Get reverse biological primer for a region (adapter stripped)."""
    return _strip_adapter(_region_primers.loc[region, 'primer_3'])

# Derive reference database from taxonomy_database config (same one used by DADA2)
_active_db = [db for db in config["taxonomy_database"]
              if config["taxonomy_database"][db].get("use", False)]
if len(_active_db) == 0:
    raise ValueError("No taxonomy database has use: True in favabean.yaml")
SIDLE_DB  = _active_db[0].lower()
SIDLE_REF  = f"data/resources/{_active_db[0]}_ref.fa.gz"
SIDLE_SPEC = f"data/resources/{_active_db[0]}_species.fa.gz"

# Extract versioned database name from the download URL for reporting
# e.g. "eHOMD_RefSeq_dada2_V15.22.fasta.gz" → "eHOMD_V15.22"
# e.g. "silva_nr99_v138.1_wSpecies_train_set.fa.gz" → "silva_v138.1"
import re as _re
_db_url = config["taxonomy_database"][_active_db[0]].get("url", "")
_db_fname = _db_url.rsplit('/', 1)[-1] if _db_url else ""
_version_match = _re.search(r'[Vv]\d+(?:\.\d+)*', _db_fname)
DB_VERSION = f"{_active_db[0]}_{_version_match.group()}" if _version_match else _active_db[0]

# SIDLE-specific config (with safe defaults if sidle: section is absent)
_sidle_cfg    = config.get("sidle", {})
SIDLE_MAX_MM  = _sidle_cfg.get("max_mismatch", 2)
SIDLE_MIN_CNT = _sidle_cfg.get("min_counts", 1000)
SIDLE_MIN_ABN = _sidle_cfg.get("min_abund", 1e-5)
# region_normalize is now a YAML list (like trim_param / figaro); take the first element
_norm_raw = _sidle_cfg.get("region_normalize", ["average"])
SIDLE_NORM = _norm_raw[0] if isinstance(_norm_raw, list) else _norm_raw

# If multiple primer regions exist, SIDLE reconstruction is triggered automatically
_sidle_inputs = (
    ["data/favabean/reconstructed_ASV.biom",
     "data/favabean/reconstructed_ASV_species.biom",
     "data/favabean/sidle/reconstructed_taxonomy.tsv"]
    if len(_unique_regions) > 1 else []
)

# ---------------------------------------------------------------------------
# SIDLE Rule 1: Convert DADA2 condense output to FASTA + count TSV per region
# ---------------------------------------------------------------------------
rule sidle_extract_asvs:
    input:
        condense = "data/favabean/{region}_condense.tsv"
    output:
        fasta      = "data/favabean/sidle/{region}/rep_seqs.fasta",
        counts     = "data/favabean/sidle/{region}/asv_counts.tsv",
        seqlengths = "data/favabean/sidle/{region}/seqlengths.txt"
    log:
        "data/logs/sidle-extract_asvs-{region}.log"
    benchmark:
        "data/benchmarks/sidle_extract_asvs-{region}.txt"
    conda:
        "../envs/dada2.yaml"
    message:
        "SIDLE [1/8] — Extracting ASV representative sequences and abundance counts from the DADA2 condensed sequence table for region {wildcards.region}. Outputs a FASTA file of unique ASVs, a sample-by-ASV count matrix, and a sequence-length summary needed for downstream trimming."
    shell:
        """
        mkdir -p data/favabean/sidle/{wildcards.region} &&
        Rscript --vanilla workflow/scripts/seqtab_to_fasta.R \
            {input.condense} {output.fasta} {output.counts} {output.seqlengths} > {log} 2>&1
        """


# ---------------------------------------------------------------------------
# SIDLE Rule 2: Trim ASVs to uniform length (required for SIDLE alignment)
# ---------------------------------------------------------------------------
rule sidle_trim_asvs:
    input:
        fasta      = "data/favabean/sidle/{region}/rep_seqs.fasta",
        counts     = "data/favabean/sidle/{region}/asv_counts.tsv",
        seqlengths = "data/favabean/sidle/{region}/seqlengths.txt"
    output:
        fasta  = "data/favabean/sidle/{region}/trimmed_seqs.fasta",
        counts = "data/favabean/sidle/{region}/trimmed_counts.tsv"
    log:
        "data/logs/sidle-trim_asvs-{region}.log"
    benchmark:
        "data/benchmarks/sidle_trim_asvs-{region}.txt"
    conda:
        "../envs/sidle.yaml"
    message:
        "SIDLE [2/8] — Trimming all ASVs in region {wildcards.region} to a uniform length. SIDLE's k-mer alignment requires fixed-length sequences so that ASVs and reference k-mers can be compared at the same positions."
    shell:
        """
        TRIM_LEN=$(head -1 {input.seqlengths}) &&
        python -m sidle_standalone.cli trim \
            {input.fasta} {input.counts} \
            {output.fasta} {output.counts} \
            --trim-length $TRIM_LEN > {log} 2>&1
        """


# ---------------------------------------------------------------------------
# SIDLE Rule 3-prep: Prepare SIDLE-ready reference from DADA2 training FASTAs
#
# DADA2 training sets use taxonomy-as-header FASTAs that often have
# non-unique headers (e.g., eHOMD has 1015 sequences but only 219 unique
# genus-level headers). read_fasta()'s dict-based storage silently drops
# duplicates. Species info lives in a separate file.
#
# This rule merges both files into a single FASTA with:
#   - Unique sequence IDs (accession numbers from species file)
#   - Species-enriched taxonomy (appended as 7th rank)
# and produces a taxonomy TSV for Rule 7.
# ---------------------------------------------------------------------------
rule sidle_prepare_reference:
    input:
        ref  = SIDLE_REF,
        spec = SIDLE_SPEC
    output:
        prepared = "data/favabean/sidle/reference/prepared_ref.fasta",
        taxonomy = "data/favabean/sidle/reference/taxonomy.tsv"
    log:
        "data/logs/sidle-prepare_reference.log"
    benchmark:
        "data/benchmarks/sidle_prepare_reference.txt"
    conda:
        "../envs/sidle.yaml"
    message:
        "SIDLE [3-prep/8] — Preparing a SIDLE-ready reference database by merging the DADA2 training-set FASTA (taxonomy-as-header, genus-level) with the species-assignment FASTA (accession + binomial). This ensures every reference sequence has a unique ID and includes species-level taxonomy, enabling SIDLE to resolve taxonomy below genus."
    shell:
        """
        mkdir -p data/favabean/sidle/reference &&
        python workflow/scripts/prepare_sidle_reference.py \
            {input.ref} {input.spec} \
            {output.prepared} {output.taxonomy} > {log} 2>&1
        """


# ---------------------------------------------------------------------------
# SIDLE Rule 3a: In-silico PCR — extract amplicon region from reference
#
# Uses skbio.DNA.to_regex() for IUPAC-degenerate-aware primer matching
# (same approach as QIIME2 feature-classifier extract-reads).
# Degenerate bases in the primer become regex character classes
# (e.g. Y->[CT], R->[AG]), so they match natively without burning
# mismatch budget. Extracts the amplicon between the primers.
# ---------------------------------------------------------------------------
rule sidle_extract_ref:
    input:
        ref = "data/favabean/sidle/reference/prepared_ref.fasta"
    output:
        extracted = "data/favabean/sidle/reference/{region}_extracted.fasta"
    params:
        fwd_primer = lambda wildcards: _get_fwd_primer(wildcards.region),
        rev_primer = lambda wildcards: _get_rev_primer(wildcards.region),
        max_mismatch = _sidle_cfg.get("primer_max_mismatch", 1)
    log:
        "data/logs/sidle-extract_ref-{region}.log"
    benchmark:
        "data/benchmarks/sidle_extract_ref-{region}.txt"
    conda:
        "../envs/sidle.yaml"
    message:
        "SIDLE [3a/8] — In-silico PCR: using the {wildcards.region} primer pair to extract the corresponding amplicon region from the full-length reference database. Degenerate bases in the primers are expanded to regex character classes (via skbio) so that ambiguous positions match natively."
    shell:
        """
        python -c "
import re, gzip, sys
from skbio import DNA
from Bio import SeqIO

fwd_str = '{params.fwd_primer}'
rev_str = '{params.rev_primer}'
fwd = DNA(fwd_str)
rev_rc = DNA(rev_str).reverse_complement()
fwd_pat = re.compile(fwd.to_regex().pattern, re.IGNORECASE)
rev_pat = re.compile(rev_rc.to_regex().pattern, re.IGNORECASE)

# Build truncated forward patterns for cases where the reference
# sequence starts inside the primer binding site (primer overhangs
# the 5-prime end). Trim up to half the primer from the 5-prime end.
min_fwd = max(8, len(fwd_str) // 2)
fwd_trunc = []
for k in range(1, len(fwd_str) - min_fwd + 1):
    sub = DNA(fwd_str[k:])
    pat = re.compile('^' + sub.to_regex().pattern, re.IGNORECASE)
    fwd_trunc.append((k, pat))

handle = gzip.open('{input.ref}', 'rt') if '{input.ref}'.endswith('.gz') else open('{input.ref}')
import os; os.makedirs(os.path.dirname('{output.extracted}'), exist_ok=True)
n_in = n_out = 0
with open('{output.extracted}', 'w') as out:
    for rec in SeqIO.parse(handle, 'fasta'):
        n_in += 1
        seq = str(rec.seq)
        # Try full forward primer first
        fwd_m = fwd_pat.search(seq)
        if fwd_m:
            amp_start = fwd_m.end()
        else:
            # Try truncated primers anchored at sequence start
            amp_start = None
            for k, pat in fwd_trunc:
                m = pat.match(seq)
                if m:
                    amp_start = m.end()
                    break
            if amp_start is None:
                continue
        # Search for reverse primer after forward match
        rev_m = rev_pat.search(seq, amp_start)
        if not rev_m:
            continue
        amplicon = seq[amp_start:rev_m.start()]
        if len(amplicon) < 50:
            continue
        # Use rec.description (full header) and normalize spaces after
        # semicolons so BioPython rec.id won't truncate species names.
        # Handles eHOMD ('; species'), Greengenes ('; p__'), SILVA (no spaces).
        seq_id = rec.description.replace('; ', ';')
        out.write('>' + seq_id + chr(10) + amplicon + chr(10))
        n_out += 1
handle.close()
print(f'Extracted {{n_out}}/{{n_in}} sequences for region {wildcards.region}')
if n_out == 0:
    print('ERROR: No sequences matched primers', file=sys.stderr)
    sys.exit(1)
" > {log} 2>&1
        """


# ---------------------------------------------------------------------------
# SIDLE Rule 3b: Prepare k-mers from the extracted regional reference
# ---------------------------------------------------------------------------
rule sidle_prepare_database:
    input:
        ref        = "data/favabean/sidle/reference/{region}_extracted.fasta",
        seqlengths = "data/favabean/sidle/{region}/seqlengths.txt"
    output:
        kmers    = "data/favabean/sidle/reference/{region}_kmers.fasta",
        kmer_map = "data/favabean/sidle/reference/{region}_kmer_map.tsv"
    params:
        fwd_primer  = lambda wildcards: _get_fwd_primer(wildcards.region),
        rev_primer  = lambda wildcards: _get_rev_primer(wildcards.region)
    log:
        "data/logs/sidle-prepare_db-{region}.log"
    benchmark:
        "data/benchmarks/sidle_prepare_database-{region}.txt"
    conda:
        "../envs/sidle.yaml"
    message:
        "SIDLE [3b/8] — Splitting the extracted {wildcards.region} reference amplicons into fixed-length k-mers that match the ASV trim length. Produces a k-mer FASTA for alignment and a map linking each k-mer back to its parent reference sequence and genomic position."
    shell:
        """
        mkdir -p data/favabean/sidle/reference &&
        TRIM_LEN=$(head -1 {input.seqlengths}) &&
        python -m sidle_standalone.cli extract \
            {input.ref} {wildcards.region} $TRIM_LEN \
            {params.fwd_primer} {params.rev_primer} \
            {output.kmers} {output.kmer_map} > {log} 2>&1
        """


# ---------------------------------------------------------------------------
# SIDLE Rule 4: Align trimmed ASVs to reference k-mers per region
# ---------------------------------------------------------------------------
rule sidle_align:
    input:
        kmers = "data/favabean/sidle/reference/{region}_kmers.fasta",
        asvs  = "data/favabean/sidle/{region}/trimmed_seqs.fasta"
    output:
        alignment = "data/favabean/sidle/{region}/alignment.tsv"
    params:
        max_mismatch = SIDLE_MAX_MM
    log:
        "data/logs/sidle-align-{region}.log"
    benchmark:
        "data/benchmarks/sidle_align-{region}.txt"
    conda:
        "../envs/sidle.yaml"
    message:
        "SIDLE [4/8] — Aligning trimmed ASVs from region {wildcards.region} against reference k-mers (max {params.max_mismatch} mismatches). Each ASV is matched to one or more reference k-mers, establishing which reference sequences could have generated the observed ASV."
    shell:
        """
        python -m sidle_standalone.cli align \
            {input.kmers} {input.asvs} {wildcards.region} \
            {output.alignment} > {log} 2>&1
        """


# ---------------------------------------------------------------------------
# SIDLE Rule 5: Build database map (combines all regions)
# ---------------------------------------------------------------------------
rule sidle_build_db_map:
    input:
        alignments = expand("data/favabean/sidle/{region}/alignment.tsv", region=SIDLE_REGIONS),
        kmer_maps  = expand("data/favabean/sidle/reference/{region}_kmer_map.tsv", region=SIDLE_REGIONS)
    output:
        db_map     = "data/favabean/sidle/database_map.tsv",
        db_summary = "data/favabean/sidle/database_summary.tsv",
        regions_list     = temp("data/favabean/sidle/_regions.txt"),
        alignments_list  = temp("data/favabean/sidle/_alignments.txt"),
        kmer_maps_list   = temp("data/favabean/sidle/_kmer_maps.txt")
    params:
        regions_str     = "\n".join(SIDLE_REGIONS),
        alignments_str  = lambda wildcards, input: "\n".join(input.alignments),
        kmer_maps_str   = lambda wildcards, input: "\n".join(input.kmer_maps)
    log:
        "data/logs/sidle-build_db_map.log"
    benchmark:
        "data/benchmarks/sidle_build_db_map.txt"
    conda:
        "../envs/sidle.yaml"
    message:
        "SIDLE [5/8] — Building a cross-region database map by combining k-mer alignments and reference maps from all regions ({SIDLE_REGIONS}). This identifies which full-length reference sequences are supported by ASV evidence across multiple primer regions, resolving ambiguities that exist within any single region."
    shell:
        """
        printf '{params.regions_str}\n' > {output.regions_list}
        printf '{params.alignments_str}\n' > {output.alignments_list}
        printf '{params.kmer_maps_str}\n' > {output.kmer_maps_list}
        python -m sidle_standalone.cli build-db \
            {output.regions_list} {output.alignments_list} {output.kmer_maps_list} \
            {output.db_map} {output.db_summary} > {log} 2>&1
        """


# ---------------------------------------------------------------------------
# SIDLE Rule 6: EM reconstruction (combines all regions)
# ---------------------------------------------------------------------------
rule sidle_reconstruct:
    input:
        alignments = expand("data/favabean/sidle/{region}/alignment.tsv", region=SIDLE_REGIONS),
        counts     = expand("data/favabean/sidle/{region}/trimmed_counts.tsv", region=SIDLE_REGIONS),
        db_map     = "data/favabean/sidle/database_map.tsv",
        db_summary = "data/favabean/sidle/database_summary.tsv"
    output:
        recon_counts     = "data/favabean/sidle/reconstructed_counts.tsv",
        regions_list     = temp("data/favabean/sidle/_recon_regions.txt"),
        alignments_list  = temp("data/favabean/sidle/_recon_alignments.txt"),
        counts_list      = temp("data/favabean/sidle/_recon_counts.txt")
    params:
        min_counts      = SIDLE_MIN_CNT,
        min_abund       = SIDLE_MIN_ABN,
        norm            = SIDLE_NORM,
        regions_str     = "\n".join(SIDLE_REGIONS),
        alignments_str  = lambda wildcards, input: "\n".join(input.alignments),
        counts_str      = lambda wildcards, input: "\n".join(input.counts)
    log:
        "data/logs/sidle-reconstruct.log"
    benchmark:
        "data/benchmarks/sidle_reconstruct.txt"
    conda:
        "../envs/sidle.yaml"
    message:
        "SIDLE [6/8] — Running the SMURF expectation-maximization (EM) algorithm to reconstruct a unified abundance table from all regions ({SIDLE_REGIONS}). The EM iteratively assigns ambiguous ASV counts to reference sequences by leveraging cross-region evidence until convergence (min counts: {params.min_counts}, min abundance: {params.min_abund}, normalization: {params.norm})."
    shell:
        """
        printf '{params.regions_str}\n' > {output.regions_list}
        printf '{params.alignments_str}\n' > {output.alignments_list}
        printf '{params.counts_str}\n' > {output.counts_list}
        python -m sidle_standalone.cli reconstruct \
            {output.regions_list} {output.alignments_list} {output.counts_list} \
            {input.db_map} {input.db_summary} {output.recon_counts} > {log} 2>&1
        """


# ---------------------------------------------------------------------------
# SIDLE Rule 7: Reconstruct taxonomy
#
# NOTE: The taxonomy TSV (reference/taxonomy.tsv) is now produced by
# Rule 3-prep (sidle_prepare_reference), which merges species info from
# the species FASTA and assigns unique accession-based IDs. The old
# Rule 7a (sidle_extract_taxonomy) has been removed.
# ---------------------------------------------------------------------------
rule sidle_assign_taxonomy:
    input:
        db_map   = "data/favabean/sidle/database_map.tsv",
        taxonomy = "data/favabean/sidle/reference/taxonomy.tsv"
    output:
        recon_tax = "data/favabean/sidle/reconstructed_taxonomy.tsv"
    params:
        database = SIDLE_DB
    log:
        "data/logs/sidle-taxonomy.log"
    benchmark:
        "data/benchmarks/sidle_assign_taxonomy.txt"
    conda:
        "../envs/sidle.yaml"
    message:
        "SIDLE [7/8] — Assigning taxonomy to reconstructed features using the {params.database} reference database. Each reconstructed feature may map to multiple reference sequences; taxonomy is determined by finding the lowest common ancestor (LCA) across all contributing references, weighted by the EM-derived assignments. Species-level resolution is possible when all contributing references for a feature share the same species."
    shell:
        """
        python -m sidle_standalone.cli taxonomy \
            {input.db_map} {input.taxonomy} {output.recon_tax} \
            --database {params.database} > {log} 2>&1
        """


# ---------------------------------------------------------------------------
# SIDLE Rule 8: Merge counts + taxonomy and convert to BIOM format
#
# Produces TWO output tables:
#   - reconstructed_ASV         : accession-level (one row per reference
#                                 sequence / clade), preserving the full
#                                 granularity of the SMURF EM estimates.
#   - reconstructed_ASV_species : species-condensed (rows that share the
#                                 same taxonomy string are merged by summing
#                                 their counts).
# Both are written as TSV and BIOM.
# ---------------------------------------------------------------------------
rule sidle_to_biom:
    input:
        counts   = "data/favabean/sidle/reconstructed_counts.tsv",
        taxonomy = "data/favabean/sidle/reconstructed_taxonomy.tsv"
    output:
        accession_tsv  = "data/favabean/reconstructed_ASV.tsv",
        accession_biom = "data/favabean/reconstructed_ASV.biom",
        species_tsv    = "data/favabean/reconstructed_ASV_species.tsv",
        species_biom   = "data/favabean/reconstructed_ASV_species.biom"
    log:
        "data/logs/sidle-to_biom.log"
    benchmark:
        "data/benchmarks/sidle_to_biom.txt"
    conda:
        "../envs/biom.yaml"
    message:
        "SIDLE [8/8] — Merging the reconstructed abundance table with taxonomy assignments into two output tables: (1) accession-level (one row per reference clade) and (2) species-condensed (rows sharing the same taxonomy summed together). Both are saved as TSV and BIOM."
    shell:
        """
        python -c "
import pandas as pd

counts = pd.read_csv('{input.counts}', sep=chr(9), index_col=0)
tax = pd.read_csv('{input.taxonomy}', sep=chr(9), index_col=0)

# Assign taxonomy to each feature
counts['taxonomy'] = tax.reindex(counts.index).iloc[:, 0].fillna('')
sample_cols = [c for c in counts.columns if c != 'taxonomy']

# --- Accession-level table (full EM resolution) ---
acc = counts.copy()
acc.index.name = '#OTU ID'
acc.to_csv('{output.accession_tsv}', sep=chr(9))
n_acc = len(acc)
print(f'Accession-level: {{n_acc}} features')

# --- Species-condensed table (sum counts for identical taxonomy) ---
condensed = counts.groupby('taxonomy', sort=False)[sample_cols].sum()
condensed.index.name = '#OTU ID'
condensed['taxonomy'] = condensed.index
condensed.to_csv('{output.species_tsv}', sep=chr(9))
n_cond = len(condensed)
print(f'Species-condensed: {{n_acc}} features -> {{n_cond}} unique taxa')
" > {log} 2>&1 &&
        biom convert \
            -i {output.accession_tsv} \
            -o {output.accession_biom} \
            --to-json \
            --table-type="OTU table" \
            --process-obs-metadata taxonomy >> {log} 2>&1 &&
        biom convert \
            -i {output.species_tsv} \
            -o {output.species_biom} \
            --to-json \
            --table-type="OTU table" \
            --process-obs-metadata taxonomy >> {log} 2>&1
        """


# =============================================================================
# Final pipeline rules
# =============================================================================

rule paired_taxonomy:
    input:
        expand("data/favabean/{region}_{db}_ASV.tsv",region=[combo[1] for combo in combinations],db=[db for db in config["taxonomy_database"] if config["taxonomy_database"][db].get("use", False)]),
        *_sidle_inputs
    output:
        touch("data/favabean/.done_biom_convert.txt"),
        preprocess_summary = "data/favabean/preprocessing_summary.tsv",
        benchmark_summary = "data/benchmarks/benchmark_summary.tsv",
        benchmark_summary_favabean = "data/favabean/benchmark_summary.tsv"
    log:
        "data/logs/biom_convert.log"
    benchmark:
        "data/benchmarks/paired_taxonomy.txt"
    conda:
        "../envs/biom.yaml"
    message: "Converting per-region ASV and taxonomy tables to BIOM format, and performing preprocess and benchmark summaries"
    shell:
        """
        ls data/favabean/*_*_ASV.tsv | parallel 'biom convert -i {{}} -o {{.}}.biom --to-json --table-type="OTU table" --process-obs-metadata taxonomy' > {log} 2>&1
        ls data/favabean/*_*_taxonomy.tsv | parallel 'biom convert -i {{}} -o {{.}}.biom --to-json --table-type="OTU table"' >> {log} 2>&1
        Rscript --vanilla workflow/scripts/preprocessing_summary.R {output.preprocess_summary} {DB_VERSION} > data/logs/preprocessing_summary.log 2>&1
        Rscript --vanilla workflow/scripts/compile_benchmarks.R {output.benchmark_summary} > data/logs/compile_benchmarks.log 2>&1
        cp {output.benchmark_summary} {output.benchmark_summary_favabean}
        cp $(echo {output.benchmark_summary} | sed 's/\\.tsv$/_detail.tsv/') $(echo {output.benchmark_summary_favabean} | sed 's/\\.tsv$/_detail.tsv/')
        """

rule paired:
    input:
        expand("data/favabean/{region}_condense.tsv",region=[combo[1] for combo in combinations])
