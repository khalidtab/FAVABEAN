import os
import pandas as pd

# Path to the output CSV file generated by the R script
output_csv = "data/files_info_Batches.csv"

########################################
# Rule to generate the CSV file
########################################
rule generate_files_info_csv:
    output:
        output_csv
    conda:
        "../envs/dada2.yaml"  # Ensure this environment contains necessary R packages
    shell:
        """
        Rscript workflow/scripts/batch.R
        """

########################################
# Function to read the CSV file after it's generated
########################################
def get_samples_table():
    samples_table = pd.read_csv(output_csv).set_index(["sample", "region"], drop=False)
    # Internally adjust the `fastq1` and `fastq2` columns
    samples_table["fastq1"] = "data/" + samples_table["fastq1"]
    samples_table["fastq2"] = "data/" + samples_table["fastq2"]

    # Check for column names with leading or trailing whitespaces
    columns = samples_table.columns
    stripped_columns = [col.strip() for col in columns]
    whitespace_columns = [col for col, stripped_col in zip(columns, stripped_columns) if col != stripped_col]
    if whitespace_columns:
        print(f"Warning: The following columns have leading or trailing whitespaces: {whitespace_columns}")

    return samples_table

########################################
# Function to get samples from batch and region
########################################
def get_samples_from_batch_region(batch, region):
    samples_table = get_samples_table()
    return samples_table.loc[(samples_table['Batch_ID'] == batch) & (samples_table['region'] == region), 'sample'].tolist()

########################################
# Define thread management functions
########################################
def determine_threads(wildcards):
    total_cores = workflow.cores
    return max(1, total_cores // 2)

def all_threads(wildcards):
    return max(1, workflow.cores)

########################################
# Function to get combinations of batches and regions
########################################
def get_combinations():
    samples_table = get_samples_table()
    combinations = samples_table.drop_duplicates(['Batch_ID', 'region'])[['Batch_ID', 'region']]
    return [tuple(x) for x in combinations.values]

########################################
# Rule cutadapt
########################################
rule cutadapt:
    input:
        fq1 = lambda wildcards: get_samples_table().loc[(wildcards.sample, wildcards.region), 'fastq1'],
        fq2 = lambda wildcards: get_samples_table().loc[(wildcards.sample, wildcards.region), 'fastq2'],
        csv = output_csv  # Ensure dependency on the generated CSV
    output:
        touch("data/favabean/{batch}-{region}/.tmp/.{sample}-cutadapt.done")
    log:
        "data/logs/cutadapt-{batch}-{region}-{sample}.log"
    params:
        primer_5 = lambda wildcards: get_samples_table().loc[(wildcards.sample, wildcards.region), 'primer_5'],
        primer_3 = lambda wildcards: get_samples_table().loc[(wildcards.sample, wildcards.region), 'primer_3'],
        sampleNum = lambda wildcards: get_samples_table().loc[(wildcards.sample, wildcards.region), 'SampleNum'],
        filt = config["initial_filter"]
    message: "Cutadapt - Removing adaptors for sample {wildcards.sample}, batch: {wildcards.batch}, region: {wildcards.region}"
    conda:
        "../envs/cutadapt.yaml"
    shell:
        """
        primer5=$(echo '{params.primer_5}' | sed 's/;/ -g /g')
        primer3=$(echo '{params.primer_3}' | sed 's/;/ -G /g')
        mkdir -p data/favabean/{wildcards.batch}-{wildcards.region}/initialFilt_discarded/ &&
        mkdir -p data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/ &&
        cutadapt -j 0 --minimum-length {params.filt} -g $primer5 -G $primer3 \
        --too-short-output        data/favabean/{wildcards.batch}-{wildcards.region}/initialFilt_discarded/{wildcards.sample}_S{params.sampleNum}_L001_R1_001.fastq.gz \
        --too-short-paired-output data/favabean/{wildcards.batch}-{wildcards.region}/initialFilt_discarded/{wildcards.sample}_S{params.sampleNum}_L001_R2_001.fastq.gz \
        -o data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/{wildcards.sample}_S{params.sampleNum}_L001_R1_001.fastq.gz \
        -p data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/{wildcards.sample}_S{params.sampleNum}_L001_R2_001.fastq.gz \
        {input.fq1} {input.fq2} > {log} 2>&1 
        """

########################################
# Rule sequence_length_stats
########################################
rule sequence_length_stats:
    input:
        sample_done=lambda wildcards: expand(
            "data/favabean/{batch}-{region}/.tmp/.{sample}-cutadapt.done",
            batch=wildcards.batch,
            region=wildcards.region,
            sample=get_samples_from_batch_region(wildcards.batch, wildcards.region)
        ),
        csv = output_csv
    output:
        R1="data/favabean/{batch}-{region}/.trimParamR1.txt",
        R2="data/favabean/{batch}-{region}/.trimParamR2.txt"
    params:
        filt=config["initial_filter"],
        trim_param=config["trim_param"]
    message: "SeqKit - Calculate statistics of sequence lengths for batch: {wildcards.batch}, region: {wildcards.region}"
    conda:
        "../envs/seqkit.yaml"
    shell:
        """
        #R1
            echo {params.trim_param} > data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/.R1{params.trim_param}.txt
            cat data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/*R1_001.fastq.gz | gzip -d | seqkit stats > data/favabean/{wildcards.batch}-{wildcards.region}/stats_R1_lengths.txt -T -a
            
            values=$(tail -n 1 data/favabean/{wildcards.batch}-{wildcards.region}/stats_R1_lengths.txt | cut -f7-11)
            echo $values | cut -d' ' -f2 > data/favabean/{wildcards.batch}-{wildcards.region}/.R1_max_len.txt
            echo $values | cut -d' ' -f3 > data/favabean/{wildcards.batch}-{wildcards.region}/.R1_Q1.txt
            echo $values | cut -d' ' -f4 > data/favabean/{wildcards.batch}-{wildcards.region}/.R1_Q2.txt
            echo $values | cut -d' ' -f5 > data/favabean/{wildcards.batch}-{wildcards.region}/.R1_Q3.txt
            
            # Additional processing to determine default value
            x_int=$(printf "%.0f\n" $(cat data/favabean/{wildcards.batch}-{wildcards.region}/.R1_Q1.txt))
            y_int=$(printf "%.0f\n" $(cat data/favabean/{wildcards.batch}-{wildcards.region}/.R1_Q2.txt))
            ten_percent=$((y_int / 10))
            lower_bound=$((y_int - ten_percent))
            upper_bound=$((y_int + ten_percent))
            if [ "$x_int" -ge "$lower_bound" ] && [ "$x_int" -le "$upper_bound" ]; then
                echo $x_int > data/favabean/{wildcards.batch}-{wildcards.region}/.R1_default.txt
            else
                echo $y_int > data/favabean/{wildcards.batch}-{wildcards.region}/.R1_default.txt
            fi
            cp data/favabean/{wildcards.batch}-{wildcards.region}/.R1_{params.trim_param}.txt data/favabean/{wildcards.batch}-{wildcards.region}/.trimParamR1.txt
            
            #R2
            echo {params.trim_param} > data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/.R2_{params.trim_param}.txt
            cat data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/*R2_001.fastq.gz | gzip -d | seqkit stats > data/favabean/{wildcards.batch}-{wildcards.region}/stats_R2_lengths.txt -T -a
            
            values=$(tail -n 1 data/favabean/{wildcards.batch}-{wildcards.region}/stats_R2_lengths.txt | cut -f7-11)
            echo $values | cut -d' ' -f2 > data/favabean/{wildcards.batch}-{wildcards.region}/.R2_max_len.txt
            echo $values | cut -d' ' -f3 > data/favabean/{wildcards.batch}-{wildcards.region}/.R2_Q1.txt
            echo $values | cut -d' ' -f4 > data/favabean/{wildcards.batch}-{wildcards.region}/.R2_Q2.txt
            echo $values | cut -d' ' -f5 > data/favabean/{wildcards.batch}-{wildcards.region}/.R2_Q3.txt
            
            # Additional processing to determine default value
            x_int=$(printf "%.0f\n" $(cat data/favabean/{wildcards.batch}-{wildcards.region}/.R2_Q1.txt))
            y_int=$(printf "%.0f\n" $(cat data/favabean/{wildcards.batch}-{wildcards.region}/.R2_Q2.txt))
            ten_percent=$((y_int / 10))
            lower_bound=$((y_int - ten_percent))
            upper_bound=$((y_int + ten_percent))
            if [ "$x_int" -ge "$lower_bound" ] && [ "$x_int" -le "$upper_bound" ]; then
                echo $x_int > data/favabean/{wildcards.batch}-{wildcards.region}/.R2_default.txt
            else
                echo $y_int > data/favabean/{wildcards.batch}-{wildcards.region}/.R2_default.txt
            fi
            cp data/favabean/{wildcards.batch}-{wildcards.region}/.R2_{params.trim_param}.txt data/favabean/{wildcards.batch}-{wildcards.region}/.trimParamR2.txt
        """

########################################
# Rule cutAndKeepSameLengthSequencesForFigaro
########################################
rule cutAndKeepSameLengthSequencesForFigaro:
    input:
        sampleCutadapt=lambda wildcards: expand(
            "data/favabean/{batch}-{region}/.tmp/.{sample}-cutadapt.done",
            batch=wildcards.batch,
            region=wildcards.region,
            sample=get_samples_from_batch_region(wildcards.batch, wildcards.region)
        ),
        trimParamR1="data/favabean/{batch}-{region}/.trimParamR1.txt",
        trimParamR2="data/favabean/{batch}-{region}/.trimParamR2.txt",
        csv = output_csv
    output:
        touch("data/favabean/{batch}-{region}/.{sample}_seqkit.done")
    log:
        "data/logs/seqkit-{batch}-{region}-{sample}.log"
    message: "SeqKit - Filtering sample {wildcards.sample} based on length parameters"
    conda:
        "../envs/seqkit.yaml"
    params:
        sampleNum = lambda wildcards: get_samples_table().loc[(wildcards.sample, wildcards.region), 'SampleNum']
    shell:
        """
        
        mkdir -p data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/filteredForFigaro/{wildcards.sample}
        
        trimParamR1=$(cat {input.trimParamR1})
        trimParamR2=$(cat {input.trimParamR2})
        
        # Cut sequences longer than the chosen trim parameter length, then remove gaps and any sequences shorter than that, then pair the sequences so that only those that are present in both would be kept
        cat data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/{wildcards.sample}_S{params.sampleNum}_L001_R1_001.fastq.gz | gzip -d | seqkit subseq -r 1:$trimParamR1 | seqkit seq --remove-gaps -m $trimParamR1 -M $trimParamR1 > data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/{wildcards.sample}_S{params.sampleNum}_L001_R1_trimmed.fastq
        cat data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/{wildcards.sample}_S{params.sampleNum}_L001_R2_001.fastq.gz | gzip -d | seqkit subseq -r 1:$trimParamR2 | seqkit seq --remove-gaps -m $trimParamR2 -M $trimParamR2 > data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/{wildcards.sample}_S{params.sampleNum}_L001_R2_trimmed.fastq
        seqkit pair -1 data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/{wildcards.sample}_S{params.sampleNum}_L001_R1_trimmed.fastq -2 data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/{wildcards.sample}_S{params.sampleNum}_L001_R2_trimmed.fastq -O data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/filteredForFigaro/{wildcards.sample} --force > {log} 2>&1
        # I think what seqkit pair kept on rewriting the whole folder at every execution, hence why I have each sample write their result in a separate folder. Now let's move its contents output
        mv data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/filteredForFigaro/{wildcards.sample}/* data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/filteredForFigaro/
        rm -r data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/filteredForFigaro/{wildcards.sample}/
        rm data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/{wildcards.sample}_S{params.sampleNum}_L001_R1_trimmed.fastq data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/{wildcards.sample}_S{params.sampleNum}_L001_R2_trimmed.fastq
        """

########################################
# Rule figaro
########################################
rule figaro:
    input:
        seqkit_done=lambda wildcards: expand(
            "data/favabean/{batch}-{region}/.{sample}_seqkit.done",
            batch=wildcards.batch,
            region=wildcards.region,
            sample=get_samples_from_batch_region(wildcards.batch, wildcards.region)
        ),
        csv = output_csv
    output:
        "data/favabean/{batch}-{region}/figaro/trimParameters.json"
    log:
        "data/logs/figaro-{batch}-{region}.log"
    conda:
        "../envs/figaro.yaml"
    message: "Figaro - Calculating optimal trimming parameters"
    threads:
        determine_threads
    shell:
        """
        python workflow/envs/figaro/figaro/figaro.py -i data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/filteredForFigaro -o data/favabean/{wildcards.batch}-{wildcards.region}/figaro/ -f 1 -r 1 -a 500 -F illumina >> {log}
        rm -rf data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt/filteredForFigaro
        """

########################################
# Rule dada2_1_filterTrim
########################################
rule dada2_1_filterTrim:
    input:
        "data/favabean/{batch}-{region}/figaro/trimParameters.json",
        csv = output_csv
    output:
        trimFilter=directory("data/favabean/{batch}-{region}/dada2"),
        donefile  = touch("data/favabean/{batch}-{region}/.tmp/.DADA2_trimFilter.done")
    log:
        "data/logs/dada2-1trimFilter-{batch}-{region}.log"
    conda:
        "../envs/dada2.yaml"
    params:
        figaro=config["figaro"]
    message: "DADA2 - Trimming and filtering FASTQ files"
    threads:
        all_threads
    shell:
        """
        Rscript --vanilla workflow/scripts/dada2_1filterAndTrim.R -i {input} -f data/favabean/{wildcards.batch}-{wildcards.region}/cutadapt -o data/favabean/{wildcards.batch}-{wildcards.region}/dada2 -p {params.figaro} -c {threads} >> {log} 2>&1
        """

########################################
# Rule dada2_2_learnErrors_R1
########################################
rule dada2_2_learnErrors_R1:
    input:
        "data/favabean/{batch}-{region}/dada2",
        csv = output_csv
    output:
        "data/favabean/{batch}-{region}/dada2_DADA2Errors-R1.RData"
    log:
        "data/logs/dada2-2learnErrors-{batch}-{region}-R1.log"
    conda:
        "../envs/dada2.yaml"
    params:
        R="R1"
    threads:
        all_threads
    message: "DADA2 - Learning errors for R1"
    shell:
        """
        Rscript --vanilla workflow/scripts/dada2_2LearnErrors.R -i {input} -r {params.R} -c {threads} >> {log} 2>&1
        """

########################################
# Rule dada2_2_learnErrors_R2
########################################
use rule dada2_2_learnErrors_R1 as dada2_2_learnErrors_R2 with:
    output:
        "data/favabean/{batch}-{region}/dada2_DADA2Errors-R2.RData"
    log:
        "data/logs/dada2-2learnErrors-{batch}-{region}-R2.log"
    params:
        R="R2"
    message: "DADA2 - Learning errors for R2"

########################################
# Rule dada2_3_denoise_R1
########################################
rule dada2_3_denoise_R1:
    input:
        "data/favabean/{batch}-{region}/dada2_DADA2Errors-R1.RData",
        csv = output_csv
    output:
        "data/favabean/{batch}-{region}/dada2_DADA2Denoise-R1.RData"
    log:
        "data/logs/dada2-3denoise-{batch}-{region}-R1.log"
    conda:
        "../envs/dada2.yaml"
    params:
        R="R1"
    message: "DADA2 - Denoising R1"
    threads:
        all_threads
    shell:
        """
        Rscript --vanilla workflow/scripts/dada2_3denoise.R -i {input} -c {threads} -r {params.R} >> {log} 2>&1
        """

########################################
# Rule dada2_3_denoise_R2
########################################
use rule dada2_3_denoise_R1 as dada2_3_denoise_R2 with:
    input:
        "data/favabean/{batch}-{region}/dada2_DADA2Errors-R2.RData",
        csv = output_csv
    output:
        "data/favabean/{batch}-{region}/dada2_DADA2Denoise-R2.RData"
    log:
        "data/logs/dada2-3denoise-{batch}-{region}-R2.log"
    params:
        R="R2"
    message: "DADA2 - Denoising R2"

########################################
# Rule dada2_4_mergePairedEnds
########################################
rule dada2_4_mergePairedEnds:
    input:
        R1="data/favabean/{batch}-{region}/dada2_DADA2Denoise-R1.RData",
        R2="data/favabean/{batch}-{region}/dada2_DADA2Denoise-R2.RData",
        csv = output_csv
    output:
        "data/favabean/{batch}-{region}/seqtab.tsv"
    log:
        "data/logs/dada2-mergePairedEnds-{batch}-{region}.log"
    conda:
        "../envs/dada2.yaml"
    message: "DADA2 - Merging paired ends"
    shell:
        """
        Rscript --vanilla workflow/scripts/dada2_4mergeR1R2.R -i {input.R1} -s {input.R2} >> {log} 2>&1
        rm -rf data/favabean/{wildcards.batch}-{wildcards.region}/.tmp
        """

########################################
# Rule dada2_5_ChimeraDetectAndRemove
########################################
rule dada2_5_ChimeraDetectAndRemove:
    input:
        seqtabs=lambda wildcards: expand(
            "data/favabean/{batch}-{region}/seqtab.tsv",
            batch=[combo[0] for combo in get_combinations()],
            region=[combo[1] for combo in get_combinations()]
        ),
        csv = output_csv
    output:
        "data/favabean/{region}_chimeraRemoved.RObjects"
    log:
        "data/logs/dada2-ChimeraASVs-{region}.log"
    conda:
        "../envs/dada2.yaml"
    threads:
        all_threads
    message: "DADA2 - Removing chimeras for region: {wildcards.region}"
    shell:
        """
        Rscript --vanilla workflow/scripts/dada2_5chimera.R -i data/favabean/ -p {wildcards.region} -c {threads} -o {output} >> {log} 2>&1
        """

########################################
# Rule dada2_6_condense
########################################
rule dada2_6_condense:
    input:
        "data/favabean/{region}_chimeraRemoved.RObjects",
        csv = output_csv
    output:
        "data/favabean/{region}_ASV.tsv"
    log:
        "data/logs/dada2-condenseASVs-{region}.log"
    conda:
        "../envs/dada2.yaml"
    threads:
        determine_threads
    message: "DADA2 - Condensing ASVs for region: {wildcards.region}"
    shell:
        """
        Rscript --vanilla workflow/scripts/dada2_6condense.R -i {input} -o {output} -r {wildcards.region} >> {log} 2>&1
        """

########################################
# Function to get URLs for taxonomy databases
########################################
def get_urls(db_name, filetype):
    if config["taxonomy_database"][db_name]["use"]:
        if filetype == "ref":
            return config["taxonomy_database"][db_name]["url"]
        elif filetype == "species":
            return config["taxonomy_database"][db_name]["species"]
    return None

########################################
# Rule download_taxonomy_databases
########################################
rule download_taxonomy_databases:
    output:
        ref     = "data/resources/{db}_ref.fa.gz",
        species = "data/resources/{db}_species.fa.gz"
    params:
        ref_url = lambda wildcards: get_urls(wildcards.db, "ref"),
        species_url = lambda wildcards: get_urls(wildcards.db, "species")
    run:
        if params.ref_url:
            shell("curl -L {params.ref_url} -o {output.ref}")
        if params.species_url:
            shell("curl -L {params.species_url} -o {output.species}")

########################################
# Rule dada2_7_assignTaxonomy
########################################
rule dada2_7_assignTaxonomy:
    input:
        ASVs="data/favabean/{region}_ASV.tsv",
        ref = ancient("data/resources/{db}_ref.fa.gz"),
        spec = ancient("data/resources/{db}_species.fa.gz"),
        csv = output_csv
    output:
        OTU_table="data/favabean/{region}_{db}_OTU.tsv",
        condensed="data/favabean/{region}_{db}_OTUcondensed.tsv"
    log:
        "data/logs/dada2-{region}-{db}_taxonomy.log"
    conda:
        "../envs/dada2.yaml"
    threads:
        determine_threads
    message: "DADA2 - Assigning taxonomy using {wildcards.db} database"
    shell:
        """
        Rscript --vanilla workflow/scripts/dada2_7assignTaxonomy.R \
            -i {input.ASVs} \
            -o {output.OTU_table} \
            -d {input.ref} \
            -s {input.spec} \
            -x {output.condensed} \
            -c {threads} > {log} 2>&1
        """

########################################
# Rule paired_taxonomy
########################################
rule paired_taxonomy:
    input:
        expand(
            "data/favabean/{region}_{db}_OTU.tsv",
            region=[combo[1] for combo in get_combinations()],
            db=[db for db in config["taxonomy_database"] if config["taxonomy_database"][db].get("use", False)]
        ),
        csv = output_csv
    output:
        "data/favabean/primer_averaged.tsv"
    log:
        "data/logs/primer_averaging.log"
    conda:
        "../envs/biom.yaml"
    message: "Creating biom files and primer averaging"
    shell:
        """
        Rscript --vanilla workflow/scripts/primer_average.R > {log} 2>&1
        ls data/favabean/*OTU*.tsv | parallel 'biom convert -i {{}} -o {{.}}.biom --to-json --table-type="OTU table" --process-obs-metadata taxonomy'
        ls data/favabean/*ASV*.tsv | parallel 'biom convert -i {{}} -o {{.}}.biom --to-json --table-type="OTU table"'
        biom convert -i data/favabean/primer_averaged.tsv -o data/favabean/primer_averaged.biom --to-json --table-type="OTU table"
        """

########################################
# Final target rule
########################################
rule all:
    input:
        "data/favabean/primer_averaged.tsv"
