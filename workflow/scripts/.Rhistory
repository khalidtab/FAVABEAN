print(paste0("Primer1: ", basename(theFiles[1]),
"    primer2: ",basename(theFiles[2])))
}
primer1 = primer1 %>% select(shared)
primer2 = primer2 %>% select(shared)
taxonomy = unique(c(primer1$taxonomy,primer2$taxonomy))
test = matrix(nrow=length(taxonomy),ncol=1+length(shared)) %>% as.data.frame(.)
colnames(test) = c("OTU_ID",shared)
test[,1] = taxonomy
heightOfTable = dim(test)[1]
readAndCondense = function(primer_path,taxa_path){
primer = read_tsv(primer_path,show_col_types = FALSE) %>% as.data.frame(.)
taxa   = read_tsv(taxa_path,show_col_types = FALSE) %>% as.data.frame(.)
colnames(primer)[1] = "OTU_ID"
primer_joined = dplyr::left_join(primer,taxa) %>% .[,-which(colnames(.) %in% c("Sequences"))]
message("Condensing the sequences based on the taxonomic rank.")
primer_joined_condensed = primer_joined %>% select(-OTU_ID) %>% group_by(Taxonomy) %>% summarize(across(everything(), sum)) %>% as.data.frame(.)
return(list(primer_joined   =  as.data.frame(primer_joined),
primer_condensed=as.data.frame(primer_joined_condensed)))
}
opt = NULL
opt$output = "data/favabean/primer_averaged.tsv"
message("First primer.")
primer1 = read_tsv(theFiles[1]) %>% as.data.frame(.)
message("Second primer.")
primer2 = read_tsv(theFiles[2]) %>% as.data.frame(.)
sampleNames = unique(c(colnames(primer1), colnames(primer2)))
sampleNames = sampleNames[-which(sampleNames %in% c("taxonomy","#SampleID"))]
sharedNonSharedSamples = (c(colnames(primer1),colnames(primer2)))
shared = sharedNonSharedSamples[duplicated(sharedNonSharedSamples)]
shared = shared[-which(shared %in% c("taxonomy","#SampleID"))]
notShared = sampleNames[-which(shared %in% sampleNames)]
if(length(notShared) > 0){
warning("The following samples were found in only one primer. Printing information on the samples, and will continue primer averaging without them.")
print(notShared)
print(paste0("Primer1: ", basename(theFiles[1]),
"    primer2: ",basename(theFiles[2])))
}
primer1 = primer1 %>% select(shared)
primer2 = primer2 %>% select(shared)
taxonomy = unique(c(primer1$taxonomy,primer2$taxonomy))
taxonomy
readAndCondense = function(primer_path,taxa_path){
primer = read_tsv(primer_path,show_col_types = FALSE) %>% as.data.frame(.)
taxa   = read_tsv(taxa_path,show_col_types = FALSE) %>% as.data.frame(.)
colnames(primer)[1] = "OTU_ID"
primer_joined = dplyr::left_join(primer,taxa) %>% .[,-which(colnames(.) %in% c("Sequences"))]
message("Condensing the sequences based on the taxonomic rank.")
primer_joined_condensed = primer_joined %>% select(-OTU_ID) %>% group_by(Taxonomy) %>% summarize(across(everything(), sum)) %>% as.data.frame(.)
return(list(primer_joined   =  as.data.frame(primer_joined),
primer_condensed=as.data.frame(primer_joined_condensed)))
}
opt = NULL
opt$output = "data/favabean/primer_averaged.tsv"
message("First primer.")
primer1 = read_tsv(theFiles[1]) %>% as.data.frame(.)
message("Second primer.")
primer2 = read_tsv(theFiles[2]) %>% as.data.frame(.)
sampleNames = unique(c(colnames(primer1), colnames(primer2)))
sampleNames = sampleNames[-which(sampleNames %in% c("taxonomy","#SampleID"))]
sharedNonSharedSamples = (c(colnames(primer1),colnames(primer2)))
shared = sharedNonSharedSamples[duplicated(sharedNonSharedSamples)]
shared = shared[-which(shared %in% c("taxonomy","#SampleID"))]
notShared = sampleNames[-which(shared %in% sampleNames)]
if(length(notShared) > 0){
warning("The following samples were found in only one primer. Printing information on the samples, and will continue primer averaging without them.")
print(notShared)
print(paste0("Primer1: ", basename(theFiles[1]),
"    primer2: ",basename(theFiles[2])))
}
primer1 = primer1 %>% select(shared,"taxonomy")
primer2 = primer2 %>% select(shared,"taxonomy")
taxonomy = unique(c(primer1$tax,primer2$taxonomy))
test = matrix(nrow=length(taxonomy),ncol=1+length(shared)) %>% as.data.frame(.)
colnames(test) = c("OTU_ID",shared)
test[,1] = taxonomy
heightOfTable = dim(test)[1]
for (x in 1:heightOfTable){ # Iterate over the rows
currentTaxa = test[x,1]
rowV13 = primer1[ which(primer1$taxonomy == currentTaxa), ]
rowV13$taxonomy = NULL
rowV13$`#SampleID` = NULL
rowV45 = primer2[ which(primer2$taxonomy == currentTaxa), ]
rowV45$taxonomy = NULL
rowV45$`#SampleID` = NULL
if (sum(as.numeric(rowV45)) == 0){
test[x,] = rowV13 # If the rowV45 is empty, then add the V13 row
} else if (sum(as.numeric(rowV13)) == 0){
test[x,] = rowV45 # If the rowV13 is empty, then add the V45 row
} else { # Meaning, the taxa exists in both tables
V13count = rowV13 %>% as.numeric(.) %>% dplyr::na_if(0) %>% as.data.frame(.)
V45count = rowV45 %>% as.numeric(.) %>% dplyr::na_if(0) %>% as.data.frame(.)
meansOfRow = rowMeans(cbind(V13count,V45count),na.rm=TRUE) %>% round(.) # Meaning, if any of them has NA, then just report the normal number, then round everything
meansOfRow[is.nan(meansOfRow)] = 0 # If na is present in both, then NaN will be reported, in this case, it means both rows had zeros, so just return it back to zero
test[x,] = c(currentTaxa,meansOfRow)
}
}
readAndCondense = function(primer_path,taxa_path){
primer = read_tsv(primer_path,show_col_types = FALSE) %>% as.data.frame(.)
taxa   = read_tsv(taxa_path,show_col_types = FALSE) %>% as.data.frame(.)
colnames(primer)[1] = "OTU_ID"
primer_joined = dplyr::left_join(primer,taxa) %>% .[,-which(colnames(.) %in% c("Sequences"))]
message("Condensing the sequences based on the taxonomic rank.")
primer_joined_condensed = primer_joined %>% select(-OTU_ID) %>% group_by(Taxonomy) %>% summarize(across(everything(), sum)) %>% as.data.frame(.)
return(list(primer_joined   =  as.data.frame(primer_joined),
primer_condensed=as.data.frame(primer_joined_condensed)))
}
opt = NULL
opt$output = "data/favabean/primer_averaged.tsv"
message("First primer.")
primer1 = read_tsv(theFiles[1]) %>% as.data.frame(.)
message("Second primer.")
primer2 = read_tsv(theFiles[2]) %>% as.data.frame(.)
sampleNames = unique(c(colnames(primer1), colnames(primer2)))
sampleNames = sampleNames[-which(sampleNames %in% c("taxonomy","#SampleID"))]
sharedNonSharedSamples = (c(colnames(primer1),colnames(primer2)))
shared = sharedNonSharedSamples[duplicated(sharedNonSharedSamples)]
shared = shared[-which(shared %in% c("taxonomy","#SampleID"))]
notShared = sampleNames[-which(shared %in% sampleNames)]
if(length(notShared) > 0){
warning("The following samples were found in only one primer. Printing information on the samples, and will continue primer averaging without them.")
print(notShared)
print(paste0("Primer1: ", basename(theFiles[1]),
"    primer2: ",basename(theFiles[2])))
}
primer1 = primer1 %>% select(shared,"taxonomy")
primer2 = primer2 %>% select(shared,"taxonomy")
taxonomy = unique(c(primer1$tax,primer2$taxonomy))
test = matrix(nrow=length(taxonomy),ncol=1+length(shared)) %>% as.data.frame(.)
colnames(test) = c("OTU_ID",shared)
test[,1] = taxonomy
heightOfTable = dim(test)[1]
test[x,1]
test[x,2]
x
test[4,1]
test[5,1]
x
currentTaxa = test[x,1]
currentTaxa
rowV13 = primer1[ which(primer1$taxonomy == currentTaxa), ]
rowV13 = primer1[ which(primer1$taxonomy == currentTaxa), ]
rowV13$taxonomy = NULL
rowV13$`#SampleID` = NULL
rowV45 = primer2[ which(primer2$taxonomy == currentTaxa), ]
rowV45$taxonomy = NULL
rowV45$`#SampleID` = NULL
sum(as.numeric(rowV45))
sum(as.numeric(rowV13))
is.na(sum(as.numeric(rowV45))
)
for (x in 1:heightOfTable){ # Iterate over the rows
currentTaxa = test[x,1]
rowV13 = primer1[ which(primer1$taxonomy == currentTaxa), ]
rowV13$taxonomy = NULL
rowV13$`#SampleID` = NULL
rowV45 = primer2[ which(primer2$taxonomy == currentTaxa), ]
rowV45$taxonomy = NULL
rowV45$`#SampleID` = NULL
if (is.na(sum(as.numeric(rowV45)))){
test[x,] = rowV13 # If the rowV45 is empty, then add the V13 row
} else if (is.na(sum(as.numeric(rowV13)))){
test[x,] = rowV45 # If the rowV13 is empty, then add the V45 row
} else { # Meaning, the taxa exists in both tables
V13count = rowV13 %>% as.numeric(.) %>% dplyr::na_if(0) %>% as.data.frame(.)
V45count = rowV45 %>% as.numeric(.) %>% dplyr::na_if(0) %>% as.data.frame(.)
meansOfRow = rowMeans(cbind(V13count,V45count),na.rm=TRUE) %>% round(.) # Meaning, if any of them has NA, then just report the normal number, then round everything
meansOfRow[is.nan(meansOfRow)] = 0 # If na is present in both, then NaN will be reported, in this case, it means both rows had zeros, so just return it back to zero
test[x,] = c(currentTaxa,meansOfRow)
}
}
test[,1] = taxonomy
test[3,]
rowSums(test[,-1])
test[,-1]
rowSums(test[,-1])
class(test$G10CB)
test[, -1] <- lapply(test[, -1], function(x) as.numeric(as.character(x)))
rowSums(test[,.1])
rowSums(test[,-1])
sort(rowSums(test[,-1]))
suppressMessages(library("optparse"))
suppressMessages(library("readr"))
suppressMessages(library("tidyr"))
suppressMessages(library("magrittr"))
suppressMessages(library("jsonlite"))
suppressMessages(library("dada2"))
jsonInput = "/Users/khaled/Desktop/sequencers_part2/michael gloguaer experimental gingivitis study  same spot multiple times/favabean/Batch1-V45/figaro/trimParameters.json" %>% jsonlite::fromJSON(.)
splitExpectedEE = as.data.frame(jsonInput$maxExpectedError) %>% as.matrix(.) %>% t(.)
colnames(splitExpectedEE) = c("R1EE","R2EE")
splitTrimPosition = as.data.frame(jsonInput$trimPosition) %>% as.matrix(.) %>% t(.)
colnames(splitTrimPosition) = c("R1Trim","R2Trim")
readRetentionPercent = jsonInput$readRetentionPercent
score = jsonInput$score
myJSONTable  =  cbind(splitExpectedEE,splitTrimPosition,readRetentionPercent,score) %>% as.data.frame(.)
inputFolder = "/Users/khaled/Desktop/sequencers_part2/michael gloguaer experimental gingivitis study  same spot multiple times/favabean/Batch1-V45/cutadapt/"
cores = 20
most_coverage = myJSONTable %>% .[which(.$readRetentionPercent == max(.$readRetentionPercent)),]
most_coverage
result_row = most_coverage[which.max(most_coverage$score),] # If there are still ties, find row with maximum score
result_row
fnFs = sort(list.files(inputFolder, pattern="R1", full.names = TRUE))
fnFs
fnRs = sort(list.files(inputFolder, pattern="R2", full.names = TRUE))
sample.names <- sapply(strsplit(basename(fnFs), "_R1_"), `[`, 1)
filtFs <- file.path(paste0(outputFolder), "filtered", paste0(sample.names, "_filt_S1_L001_R1_001.fastq.gz"))
outputFolder = "/Users/khaled/Desktop/sequencers_part2/michael gloguaer experimental gingivitis study  same spot multiple times/favabean/Batch1-V45/dada2/filtered"
filtFs <- file.path(paste0(outputFolder), "filtered", paste0(sample.names, "_filt_S1_L001_R1_001.fastq.gz"))
filtRs <- file.path(paste0(outputFolder), "filtered", paste0(sample.names, "_filt_S1_L001_R2_001.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
print("Files detected, will perform filtering and trimming on the fastq files.")
out = filterAndTrim(fnFs, filtFs, fnRs, filtRs,
truncLen=c(result_row$R1Trim,result_row$R2Trim),
maxN=0,
maxEE=c(result_row$R1EE,result_row$R2EE), truncQ=2, rm.phix=TRUE,
compress=TRUE, multithread=cores, matchIDs=TRUE)
result_row$R1Trim
?filterAndTrim
out = filterAndTrim(fnFs, filtFs, fnRs, filtRs,
truncLen=c(result_row$R1Trim,result_row$R2Trim),
maxN=0,
maxEE=c(result_row$R1EE,result_row$R2EE), truncQ=2, rm.phix=TRUE,
compress=TRUE, multithread=cores, matchIDs=TRUE,verbose = TRUE)
result_row$R1Trim
result_row$R2Trim
result_row$R1EE
result_row$R2EE
out = filterAndTrim(fnFs, filtFs, fnRs, filtRs,
truncLen=c(245,227),
maxN=0,
maxEE=c(2,4), truncQ=2, rm.phix=TRUE,
compress=TRUE, multithread=cores, matchIDs=TRUE,verbose = TRUE)
out = filterAndTrim(fnFs, filtFs, fnRs, filtRs,
truncLen=c(250,250),
maxN=0,
maxEE=c(2,4), truncQ=2, rm.phix=TRUE,
compress=TRUE, multithread=cores, matchIDs=TRUE,verbose = TRUE)
out = filterAndTrim(fnFs, filtFs, fnRs, filtRs,
truncLen=c(250,250),
maxN=0,
maxEE=c(4,4), truncQ=2, rm.phix=TRUE,
compress=TRUE, multithread=cores, matchIDs=TRUE,verbose = TRUE)
out = filterAndTrim(fnFs, filtFs, fnRs, filtRs,
truncLen=c(270,270),
maxN=0,
maxEE=c(4,4), truncQ=2, rm.phix=TRUE,
compress=TRUE, multithread=cores, matchIDs=TRUE,verbose = TRUE)
out = filterAndTrim(fnFs, filtFs, fnRs, filtRs,
truncLen=c(270,270),
maxN=0,
maxEE=c(10,10), truncQ=2, rm.phix=TRUE,
compress=TRUE, multithread=cores, matchIDs=TRUE,verbose = TRUE)
View(result_row)
out = filterAndTrim(fnFs, filtFs, fnRs, filtRs,
truncLen=c(270,270),
maxN=0,
truncQ=2, rm.phix=TRUE,
compress=TRUE, multithread=cores, matchIDs=TRUE,verbose = TRUE)
out = filterAndTrim(fnFs, filtFs, fnRs, filtRs,
maxN=0,
truncQ=2, rm.phix=TRUE,
compress=TRUE, multithread=cores, matchIDs=TRUE,verbose = TRUE)
fnFs
out = filterAndTrim(fnFs, filtFs, fnRs, filtRs,
truncLen=c(245,227),
maxN=0,
maxEE=c(2,4), truncQ=2, rm.phix=TRUE,
compress=TRUE, multithread=cores,verbose = TRUE)
suppressMessages(library("optparse"))
suppressMessages(library("readr"))
suppressMessages(library("tidyr"))
suppressMessages(library("magrittr"))
suppressMessages(library("dada2"))
outputFile = "/Users/khaled/Desktop/sequencers_part2/michael gloguaer experimental gingivitis study  same spot multiple times/favabean"
cores = 20
myparameter = "V45"
inputfiles = list.files(inputfile,pattern = "seqtab", full.names = TRUE, recursive = TRUE)
inputfile = "/Users/khaled/Desktop/sequencers_part2/michael gloguaer experimental gingivitis study  same spot multiple times/favabean"
inputfiles = list.files(inputfile,pattern = "seqtab", full.names = TRUE, recursive = TRUE)
inputfiles2 = inputfiles %>% grepl(myparameter,x=.) %>% inputfiles[.]
process_file <- function(path) {
newTable = data.table::fread(path,sep="\t") %>% as.data.frame() # This is a faster way to read the files than read_tsv()
rownames(newTable) <- newTable[, 1]
newTable[, 1] <- NULL
newTable = as.data.frame(as.matrix(newTable))
return(newTable)
}
inputfiles2 = base::sapply(inputfiles2, process_file,simplify=FALSE)
print("Merging sequence tables…")
if (length(inputfiles2) == 1){
inputfiles2 = inputfiles2[[1]] %>% as.matrix(.)
} else {
inputfiles2 = inputfiles2 %>% mergeSequenceTables(tables=.,tryRC = TRUE)
}
length(inputfiles2) == 1
inputfiles2 = inputfiles2 %>% mergeSequenceTables(tables=.,tryRC = TRUE)
process_file <- function(path) {
newTable = data.table::fread(path,sep="\t") %>% as.data.frame() # This is a faster way to read the files than read_tsv()
rownames(newTable) <- newTable[, 1]
newTable[, 1] <- NULL
newTable = as.data.frame(t(as.matrix(newTable)))
return(newTable)
}
inputfiles2 = base::sapply(inputfiles2, process_file,simplify=FALSE)
inputfiles = list.files(inputfile,pattern = "seqtab", full.names = TRUE, recursive = TRUE)
inputfiles2 = inputfiles %>% grepl(myparameter,x=.) %>% inputfiles[.]
process_file <- function(path) {
newTable = data.table::fread(path,sep="\t") %>% as.data.frame() # This is a faster way to read the files than read_tsv()
rownames(newTable) <- newTable[, 1]
newTable[, 1] <- NULL
newTable = as.data.frame(t(as.matrix(newTable)))
return(newTable)
}
inputfiles2 = base::sapply(inputfiles2, process_file,simplify=FALSE)
if (length(inputfiles2) == 1){
inputfiles2 = inputfiles2[[1]] %>% as.matrix(.)
} else {
inputfiles2 = inputfiles2 %>% mergeSequenceTables(tables=.,tryRC = TRUE)
}
inputfiles3 = inputfiles2[[1]] %>% as.matrix(.)
View(inputfiles3)
inputfiles2 = inputfiles2 %>% mergeSequenceTables(tables=.,tryRC = TRUE)
inputfiles2 = inputfiles2 %>% mergeSequenceTables(tables=as.matrix(.),tryRC = TRUE)
inputfiles2 = inputfiles %>% grepl(myparameter,x=.) %>% inputfiles[.]
process_file <- function(path) {
newTable = data.table::fread(path,sep="\t") %>% as.data.frame() # This is a faster way to read the files than read_tsv()
rownames(newTable) <- newTable[, 1]
newTable[, 1] <- NULL
newTable = t(as.matrix(newTable))
return(newTable)
}
inputfiles2 = base::sapply(inputfiles2, process_file,simplify=FALSE)
print("Merging sequence tables…")
if (length(inputfiles2) == 1){
inputfiles2 = inputfiles2[[1]] %>% as.matrix(.)
} else {
inputfiles2 = inputfiles2 %>% mergeSequenceTables(tables=as.matrix(.),tryRC = TRUE)
}
inputfiles2 = inputfiles2 %>% mergeSequenceTables(tables=.,tryRC = TRUE)
inputfiles2 = inputfiles2 %>% mergeSequenceTables(tables=.,tryRC = TRUE)
inputfiles2 = inputfiles %>% grepl(myparameter,x=.) %>% inputfiles[.]
process_file <- function(path) {
newTable = data.table::fread(path,sep="\t") %>% as.data.frame() # This is a faster way to read the files than read_tsv()
rownames(newTable) <- newTable[, 1]
newTable[, 1] <- NULL
newTable = as.matrix(newTable)
return(newTable)
}
inputfiles2 = inputfiles %>% grepl(myparameter,x=.) %>% inputfiles[.]
process_file <- function(path) {
newTable = data.table::fread(path,sep="\t") %>% as.data.frame() # This is a faster way to read the files than read_tsv()
rownames(newTable) <- newTable[, 1]
newTable[, 1] <- NULL
newTable = as.matrix(newTable)
return(newTable)
}
inputfiles2 = base::sapply(inputfiles2, process_file,simplify=FALSE)
print("Merging sequence tables…")
if (length(inputfiles2) == 1){
inputfiles2 = inputfiles2[[1]] %>% as.matrix(.)
} else {
inputfiles2 = inputfiles2 %>% mergeSequenceTables(tables=.,tryRC = TRUE)
}
suppressMessages(library("optparse"))
suppressMessages(library("readr"))
suppressMessages(library("tidyr"))
suppressMessages(library("magrittr"))
suppressMessages(library("dplyr"))
suppressMessages(library("dada2"))
inputFile = "/Users/khaled/Desktop/sequencers_part2/michael gloguaer experimental gingivitis study  same spot multiple times/favabean/V45_condense.tsv"
myDatabase = "/Users/khaled/Desktop/sequencers_part2/michael gloguaer experimental gingivitis study  same spot multiple times/resources/eHOMD_ref.fa.gz"
mySpecies = "/Users/khaled/Desktop/sequencers_part2/michael gloguaer experimental gingivitis study  same spot multiple times/resources/eHOMD_species.fa.gz"
myCores = 20
inputMatrix = read_tsv(inputFile) %>% as.data.frame(.)
ASVs = inputMatrix$SampleIDs
print("Beginning assignment up to genus level.")
taxa3 = dada2::assignTaxonomy(ASVs, myDatabase,verbose=TRUE, tryRC = TRUE, multithread = TRUE)
print("Taxonomy assignment up to genus level is completed.")
print("Beginning assignment up to species level.")
taxa4 = dada2::assignSpecies(taxa3,mySpecies,tryRC = TRUE)
print("Taxonomy assignment up to species level is completed.")
taxa3 = cbind(rownames(taxa3),taxa3) %>% as.data.frame(.)
taxa4 = cbind(rownames(taxa4),taxa4) %>% as.data.frame(.)
taxa5 = dplyr::left_join(taxa3,taxa4)
taxa5[is.na(taxa5)] = ""
taxonomy = paste0("k__",taxa5$Kingdom,"; p__",taxa5$Phylum,"; c__",taxa5$Class,"; o__",taxa5$Order,"; f__",taxa5$Family,"; g__",taxa5$Genus,"; s__",taxa5$Species)
inputMatrix2 = cbind(inputMatrix,taxonomy)
OTU_IDs = rownames(inputMatrix2)
inputMatrix3 = cbind(paste0("OTU",OTU_IDs),inputMatrix2)
colnames(inputMatrix3)[1] = "#SampleID"
taxonomy = data.frame(OTU_ID = inputMatrix3$`#SampleID`, Sequences = inputMatrix3$SampleIDs, Taxonomy = inputMatrix3$taxonomy)
inputMatrix3 = inputMatrix3 %>% .[,-which(colnames(.) %in% c("SampleIDs","taxonomy"))]
# Now let's create the different iterations of the files needed
readAndCondense = function(primer_path,taxa_path){
primer = as.data.frame(primer_path)
taxa   = as.data.frame(taxa_path)
colnames(primer)[1] = "OTU_ID"
primer_joined = dplyr::left_join(primer,taxa) %>% .[,-which(colnames(.) %in% c("Sequences"))]
colnames(primer_joined)[length(colnames(primer_joined))] = "taxonomy"
message("Condensing the sequences based on the taxonomic rank.")
primer_joined_condensed = primer_joined %>% select(-OTU_ID) %>% group_by(taxonomy) %>% summarize(across(everything(), sum)) %>% as.data.frame(.)
return(list(primer_joined   =  as.data.frame(primer_joined),
primer_condensed=as.data.frame(primer_joined_condensed)))
}
myIterations = readAndCondense(inputMatrix3,taxonomy)
inputMatrix4 = left_join(inputMatrix3,taxonomy,by = join_by(`#SampleID` == OTU_ID)) %>% select(-Sequences)
colnames(inputMatrix4)[length(colnames(inputMatrix4))] = "taxonomy"
colnames(myIterations$primer_condensed)[1] = "#SampleID"
myIterations$primer_condensed$taxonomy = myIterations$primer_condensed$`#SampleID`
myIterations$primer_condensed$`#SampleID` = paste0("OTU",1:dim(myIterations$primer_condensed)[1])
write_tsv(inputMatrix4,"../V45_eHOMD_OTU.tsv")
write_tsv(myIterations$primer_condensed,"../V45_eHOMD_taxonomy.tsv")
?write_tsv
myIterations$primer_condensed
View(myIterations)
View(myIterations[2])
View(myIterations[[2]])
View(myIterations)
myIterations[["primer_condensed"]]
suppressMessages(library("readr"))
suppressMessages(library("tidyr"))
suppressMessages(library("magrittr"))
suppressMessages(library("dplyr"))
inputFile = "theFiles = list.files("data/favabean/","_OTUcondensed.tsv",full.names = TRUE)"
inputFile = "/Users/khaled/Desktop/sequencers_part2/michael gloguaer experimental gingivitis study  same spot multiple times/favabean/V45_condense.tsv"
myDatabase = "/Users/khaled/Desktop/sequencers_part2/michael gloguaer experimental gingivitis study  same spot multiple times/favabean/V45_condense.tsv"
myDatabase = "/Users/khaled/Desktop/sequencers_part2/michael gloguaer experimental gingivitis study  same spot multiple times/resources/eHOMD_ref.fa.gz"
mySpecies = "/Users/khaled/Desktop/sequencers_part2/michael gloguaer experimental gingivitis study  same spot multiple times/resources/eHOMD_species.fa.gz"
myCores = 20
inputMatrix = read_tsv(inputFile) %>% as.data.frame(.)
ASVs = inputMatrix$SampleIDs
print("Beginning assignment up to genus level.")
taxa3 = dada2::assignTaxonomy(ASVs, myDatabase,verbose=TRUE, tryRC = TRUE, multithread = TRUE)
print("Taxonomy assignment up to genus level is completed.")
print("Beginning assignment up to species level.")
taxa4 = dada2::assignSpecies(taxa3,mySpecies,tryRC = TRUE)
print("Taxonomy assignment up to species level is completed.")
taxa3 = cbind(rownames(taxa3),taxa3) %>% as.data.frame(.)
taxa4 = cbind(rownames(taxa4),taxa4) %>% as.data.frame(.)
taxa5 = dplyr::left_join(taxa3,taxa4)
taxa5[is.na(taxa5)] = ""
taxonomy = paste0("k__",taxa5$Kingdom,"; p__",taxa5$Phylum,"; c__",taxa5$Class,"; o__",taxa5$Order,"; f__",taxa5$Family,"; g__",taxa5$Genus,"; s__",taxa5$Species)
inputMatrix2 = cbind(inputMatrix,taxonomy)
OTU_IDs = rownames(inputMatrix2)
inputMatrix3 = cbind(paste0("OTU",OTU_IDs),inputMatrix2)
colnames(inputMatrix3)[1] = "#SampleID"
taxonomy = data.frame(OTU_ID = inputMatrix3$`#SampleID`, Sequences = inputMatrix3$SampleIDs, Taxonomy = inputMatrix3$taxonomy)
inputMatrix3 = inputMatrix3 %>% .[,-which(colnames(.) %in% c("SampleIDs","taxonomy"))]
# Now let's create the different iterations of the files needed
readAndCondense = function(primer_path,taxa_path){
primer = as.data.frame(primer_path)
taxa   = as.data.frame(taxa_path)
colnames(primer)[1] = "OTU_ID"
primer_joined = dplyr::left_join(primer,taxa) %>% .[,-which(colnames(.) %in% c("Sequences"))]
colnames(primer_joined)[length(colnames(primer_joined))] = "taxonomy"
message("Condensing the sequences based on the taxonomic rank.")
primer_joined_condensed = primer_joined %>% select(-OTU_ID) %>% group_by(taxonomy) %>% summarize(across(everything(), sum)) %>% as.data.frame(.)
return(list(primer_joined   =  as.data.frame(primer_joined),
primer_condensed=as.data.frame(primer_joined_condensed)))
}
myIterations = readAndCondense(inputMatrix3,taxonomy)
inputMatrix4 = left_join(inputMatrix3,taxonomy,by = join_by(`#SampleID` == OTU_ID)) %>% select(-Sequences)
colnames(inputMatrix4)[length(colnames(inputMatrix4))] = "taxonomy"
View(inputMatrix4)
View(myIterations)
myIterations[["primer_joined"]]
View(myIterations[["primer_joined"]])
colnames(myIterations[["primer_joined"]])
View(myIterations)
View(myIterations[["primer_condensed"]])
colnames(myIterations$primer_condensed)[1] = "#SampleID"
colnames(myIterations$primer_condensed)
suppressMessages(library("readr"))
suppressMessages(library("tidyr"))
suppressMessages(library("magrittr"))
suppressMessages(library("dplyr"))
theFiles = list.files("/Users/khaled/Desktop/sequencers_part2/michael gloguaer experimental gingivitis study  same spot multiple times/favabean","_OTUcondensed.tsv",full.names = TRUE)
theFiles = list.files("data/favabean/","V45_eHOMD_taxonomy.tsv",full.names = TRUE)
theFiles = list.files("data/favabean/","_taxonomy.tsv",full.names = TRUE)
theFiles = list.files("/Users/khaled/Desktop/sequencers_part2/michael gloguaer experimental gingivitis study  same spot multiple times/favabean","_taxonomy.tsv",full.names = TRUE)
readAndCondense = function(primer_path,taxa_path){
primer = read_tsv(primer_path,show_col_types = FALSE) %>% as.data.frame(.)
taxa   = read_tsv(taxa_path,show_col_types = FALSE) %>% as.data.frame(.)
colnames(primer)[1] = "OTU_ID"
primer_joined = dplyr::left_join(primer,taxa) %>% .[,-which(colnames(.) %in% c("Sequences"))]
message("Condensing the sequences based on the taxonomic rank.")
primer_joined_condensed = primer_joined %>% select(-OTU_ID) %>% group_by(Taxonomy) %>% summarize(across(everything(), sum)) %>% as.data.frame(.)
return(list(primer_joined   =  as.data.frame(primer_joined),
primer_condensed=as.data.frame(primer_joined_condensed)))
}
suppressMessages(library("readr"))
suppressMessages(library("tidyr"))
suppressMessages(library("magrittr"))
suppressMessages(library("dplyr"))
theFiles = list.files("/Users/khaled/Desktop/sequencers_part2/michael gloguaer experimental gingivitis study  same spot multiple times/favabean","_taxonomy.tsv",full.names = TRUE)
readAndCondense = function(primer_path,taxa_path){
primer = read_tsv(primer_path,show_col_types = FALSE) %>% as.data.frame(.)
taxa   = read_tsv(taxa_path,show_col_types = FALSE) %>% as.data.frame(.)
colnames(primer)[1] = "OTU_ID"
primer_joined = dplyr::left_join(primer,taxa) %>% .[,-which(colnames(.) %in% c("Sequences"))]
message("Condensing the sequences based on the taxonomic rank.")
primer_joined_condensed = primer_joined %>% select(-OTU_ID) %>% group_by(Taxonomy) %>% summarize(across(everything(), sum)) %>% as.data.frame(.)
return(list(primer_joined   =  as.data.frame(primer_joined),
primer_condensed=as.data.frame(primer_joined_condensed)))
}
opt = NULL
opt$output = "data/favabean/primer_averaged.tsv"
message("First primer.")
primer1 = read_tsv(theFiles[1]) %>% as.data.frame(.)
message("Second primer.")
primer2 = read_tsv(theFiles[2]) %>% as.data.frame(.)
